#+title: etl Terraform in aws

This repository contains the main procedure shown for the Wizeiine DEB. This is a detailed guide to the first steps in the 101 trainee on Terraform.

* Contains of this repo

#+begin_src sh :results output :eval yes :exports results
tree
#+end_src

#+RESULTS:
: .
: ├── README.md
: ├── README.org
: ├── data
: │   └── ds_salaries.csv
: └── lamba
:
: 2 directories, 3 files

This repository only contains the main workflow in the /README/ file, inside the =/data= path there is only one file with salaries of Data Science. Inside the =/lambda= path you will find a configuration written in Python to run a lambda function which their respectively IAM permisions.

* Prerequisites
+ AWS account configured. For this example we are using default profile and set it in /us-east-2/ region
+ aws cli already set up in your computer
+ Terraform >= 0.13

* Installation
Use the package manager homebrew to install the AWS CLI.

#+begin_src sh :eval no
brew install awscli
#+end_src

After you've installed the AWS CLI, configure it by running =aws configure=.
When prompted, enter your AWS Access Key ID, Secret Access Key, region and output format.

#+begin_src sh :eval no
$ aws configure
AWS Access Key ID [None]: YOUR_AWS_ACCESS_KEY_ID
AWS Secret Access Key [None]: YOUR_AWS_SECRET_ACCESS_KEY
Default region name [None]: YOUR_AWS_REGION
Default output format [None]: json
#+end_src

If you don't have an AWS Access Credentials, create your AWS Access Key ID and Secret Access Key by navigating to your [[https://console.aws.amazon.com/iam/home?#/security_credentials][service credentials]] in the IAM service on AWS. Click "Create access key" here and download the file. This file contains your access credentials.

Your default region can be found in the AWS Web Management Console beside your username. Select the region drop down to find the region name (eg. us-east-2) corresponding with your location.

* Terraform 101
1. Following with your training, let's start runnning =touch my_first.tf= in order to create a file to puth all our first configuration inside.
2. Open your file and puth the header of the provider.

   #+begin_src terraform :eval no
   terraform {
     required_providers {
       aws = {
         source = "hashicorp/aws"
       }
     }
     required_version = ">=0.12"
   }

   provider "aws" {
     profile = "default"
     region  = "us-east-2"
   }
   #+end_src

3. Let's start creating a bucket in S3.

   #+begin_src terraform :eval no
   resource "aws_s3_bucket" "b" {
     bucket = "deb_tf_bucket"
     acl    = "private"
   }
   #+end_src

4. Deppending on what terraform version you are, you may retrive a message like this:

   #+begin_src sh :eval no
   Warning: Argument is deprecated

   with aws_s3_bucket.b,
   on my_first.tf line 17, in resource "aws_s3_bucket" "b":
   17:      acl    = "private"

   Use the aws_s3_bucket_acl resource instead
   #+end_src

   in case you have this /warning/ please change the last setup with this:

   #+begin_src terraform :eval no
   # Set the acl
   resource "aws_s3_bucket_acl" "b_acl" {
     bucket = aws_s3_bucket.b.id
     acl    = "private"
   }
   #+end_src

5. Now let's uppload our first csv file to the recently created bucket, in order to do so, please copy the next code block in your main terraform file.

   #+begin_src terraform :eval no

   # Upload an object
   resource "aws_s3_bucket_object" "object" {
   bucket   = aws_s3_bucket.b.id
     key    = "my_first_upload.csv"
     source = "data/ds_salaries.csv"
     etag   = filemd5("data/ds_salaries.csv")
   }

   #+end_src

6.
